{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Text Clustering Algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "Welcome! In this notebook, we set the neccesary functions to use the Text Clustering Algorithm that we developed earlier on [...]. As we mentioned previously, the main goal of this algorithm is to categorize the content of a website using it's plain text; this will help us to understand the kind of websites that we will visit with **TheWitness**, and also it will determine if a particular website is worth to explore. The latter is very important, since our exploration on internet growths exponentially on every step.\n",
    "\n",
    "<img src = \"./notebook_assets/nlp_1.png\" width = \"500px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "\n",
    "# Import text clustering libraries.\n",
    "import gensim\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim import corpora, models\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Import language processing libraries.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from itertools import chain\n",
    "\n",
    "# We will ignore warnings.\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LdaModel.\n",
    "lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Get stop words from english and spanish (we will include more languages\n",
    "# in the future!)\n",
    "stop = set(stopwords.words('english')).union(stopwords.words('spanish'))\n",
    "exclude = set(string.punctuation)\n",
    "\n",
    "# Initialize Lemmatizer object.\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "# We load the text clustering model that we built previously on [...].\n",
    "temp_file = datapath(\"dictio\")\n",
    "dictionary = corpora.Dictionary.load(temp_file, mmap='r')\n",
    "temp_file = datapath(\"text_clustering_model\")\n",
    "ldamodel = lda.load(temp_file, mmap='r')\n",
    "\n",
    "\"\"\"\n",
    "This functions cleans a given text, i.e, it filters stop words, and \n",
    "standardize it.\n",
    "\"\"\"\n",
    "def clean(text):\n",
    "    \n",
    "    # Filter stop words.\n",
    "    stop_free = \" \".join([word for word in text.lower().split() if word not in stop])\n",
    "    \n",
    "    # Eliminate puntuactions symbols.\n",
    "    punct_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    \n",
    "    # Lemmatize the text!\n",
    "    normalized = \" \".join([lemma.lemmatize(word) for word in punct_free.split()])\n",
    "    \n",
    "    # Return a corpus.\n",
    "    return normalized.split()\n",
    "\n",
    "\"\"\"\n",
    "This function returns the maximum probability of the distribution\n",
    "as long as the threshold is pass, othewise returns -1 (this means\n",
    "that the content is not interesting for us).\n",
    "\"\"\"\n",
    "def get_max_category(list_distribution, umbral_param):\n",
    "    \n",
    "    # Set some variables.\n",
    "    temp_label = -1\n",
    "    temp_max = 0\n",
    "    \n",
    "    # Iterate over the probability distribution.\n",
    "    for cluster in list_distribution:\n",
    "        if cluster[1] > temp_max and cluster[1] > umbral_param:\n",
    "            temp_max = cluster[1]\n",
    "            temp_label = cluster[0]\n",
    "            \n",
    "    # Return the cateogry with the maximum probability.\n",
    "    return temp_label\n",
    "\n",
    "\"\"\"\n",
    "This function generates a probability distribution over the 5 categories\n",
    "we defined previosly on the model's development notebook, and returns \n",
    "tha maximum of them, as long as they pass the threshold\n",
    "\"\"\"\n",
    "def cluster_text(text, umbral_param = 0.6):\n",
    "    \n",
    "    # Clean de text of the website.\n",
    "    clean_text = [clean(text)]\n",
    "    doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in clean_text]\n",
    "    \n",
    "    # Get the probability distribuion of the 5 categories!\n",
    "    distribution = ldamodel[doc_term_matrix_test[0]]\n",
    "    \n",
    "    # Get max category.\n",
    "    max_category = get_max_category(distribution, umbral_param)\n",
    "    return max_category\n",
    "\n",
    "def get_max_probability(text): \n",
    "    \n",
    "    # Clean de text of the website.\n",
    "    clean_text = [clean(text)]\n",
    "    doc_term_matrix_test = [dictionary.doc2bow(doc) for doc in clean_text]\n",
    "    \n",
    "    # Get the probability distribuion of the 5 categories!\n",
    "    distribution = ldamodel[doc_term_matrix_test[0]]\n",
    "    \n",
    "    # Get probability values.\n",
    "    probability_values = list()\n",
    "    for element in distribution:\n",
    "        probability_values.append(element[1])\n",
    "        \n",
    "    # Return max probability.\n",
    "    return max(probability_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Clustering Model Ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"Text Clustering Model Ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
