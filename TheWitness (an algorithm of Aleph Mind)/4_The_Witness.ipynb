{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The Witness Class.\n",
    "---\n",
    "You are stand right now in the core of **The Witness** algorithm!\n",
    "\n",
    "<img src = \"./notebook_assets/thewitness.png\" width = \"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load request libraries.\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load\n",
    "import re\n",
    "import hashlib \n",
    "import time\n",
    "import langdetect\n",
    "\n",
    "# Load natural language processing libraries.\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "import en_core_web_sm\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The witness class.\n",
    "class TheWitness: \n",
    "    \n",
    "    # Static variables.\n",
    "    headers = {}\n",
    "    headers[\"User-Agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "    social_media_names_file = \"./constants/socialmedia_names.txt\"\n",
    "    \n",
    "    # Initialize witness features.\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize requests session.\n",
    "        self.agent_session = requests.session()\n",
    "\n",
    "    # Scrap the destination website!\n",
    "    # Soup referes to a Beautiful Soup object.\n",
    "    def scrap(self, url, soup):\n",
    "        \n",
    "        # Find all the links in the current website!\n",
    "        result = self.find_website_links(url, soup)\n",
    "        internal_website_urls = result[0]\n",
    "        external_website_urls = result[1]\n",
    "        urls = self.join_lists(internal_website_urls, external_website_urls)\n",
    "        emails_list_1 = result[2]\n",
    "        social_networks_urls = result[3]\n",
    "        whatsapp_cellphones = result[4]\n",
    "        images_urls_1 = result[5]\n",
    "        \n",
    "        # Get text of the website.\n",
    "        text = self.get_text(soup)\n",
    "        \n",
    "        # Get language of the website.\n",
    "        language = self.get_language(text)\n",
    "        \n",
    "        # Load spacy model according to the language detected.\n",
    "        if language == \"en\":\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "        else:\n",
    "            nlp = spacy.load(\"es_core_news_sm\")\n",
    "            \n",
    "        # Get names that appear on the website.\n",
    "        names = self.get_names(text, nlp)\n",
    "        \n",
    "        # Get locations\n",
    "        locations = self.find_locations(text, nlp)\n",
    "        \n",
    "        # Get coordinates of maps that appear on the website.\n",
    "        iframes = self.getWebsiteIFrames(soup)\n",
    "        \n",
    "        # Get orgazations name's that appear on the website.\n",
    "        organizations = self.find_organizations(text, nlp)\n",
    "        \n",
    "        # Get dates that appear on the website.\n",
    "        dates = self.find_dates(text, nlp)\n",
    "        \n",
    "        # Get all phone numbers that appear on the website.\n",
    "        phonenumbers = self.join_lists(self.getPhoneNumbers(text),\n",
    "                                       whatsapp_cellphones)\n",
    "        \n",
    "        # Get images urls that appear on the website. \n",
    "        images_urls = self.join_lists(self.getImagesFromWebPage(soup, download=False, secs=0), \n",
    "                                      images_urls_1)\n",
    "        \n",
    "        # Get payment accounts that appear on the website.\n",
    "        payment_accounts = self.getPayment(text)\n",
    "        \n",
    "        # Get emails that appear on the website.\n",
    "        emails = self.getEmails(text)\n",
    "        \n",
    "        return [language,               # Language of the website.\n",
    "                urls,                   # URLS found on the website.\n",
    "                social_networks_urls,   # Social networks urls.\n",
    "                text,                   # Text of the website.\n",
    "                names,                  # Names that appeared on the website.\n",
    "                locations,              # Locations mentioned on website.\n",
    "                iframes,                # Iframes code (maps or videos) that appear on the website.\n",
    "                organizations,          # Organization name's.\n",
    "                dates,                  # Dates on the website.\n",
    "                phonenumbers,           # Phonenumbers on the website.\n",
    "                images_urls,            # Images urls.\n",
    "                payment_accounts,       # Payment Accounts (banks, crytpo).\n",
    "                emails                  # Emails on the website.\n",
    "               ]\n",
    "        \n",
    "    ## -----------------------\n",
    "    ## Webscrapping functions.\n",
    "    ## -----------------------\n",
    "    \n",
    "    \"\"\"\n",
    "    Return the agent's distance from the root.\n",
    "    \"\"\"\n",
    "    def getDistance(self):\n",
    "        return self.distance_from_root\n",
    "    \n",
    "    \"\"\"\n",
    "    Find google map info (if there is one).\n",
    "    \"\"\"\n",
    "    def getWebsiteIFrames(self, soup):\n",
    "        info = soup.find(\"iframe\")\n",
    "        return info\n",
    "    \n",
    "    \"\"\"\n",
    "    This function saves all the images urls and downloads them optionaly\n",
    "    \"\"\"\n",
    "    def getImagesFromWebPage(self, soup, download = False, secs = 0):\n",
    "\n",
    "        images = soup.find_all(\"img\")\n",
    "        url_images = list()\n",
    "        \n",
    "        for x in soup.find_all(\"img\"):\n",
    "            try:\n",
    "                \n",
    "                # Extra image route.\n",
    "                linkImage = x.attrs[\"src\"]\n",
    "                \n",
    "                # The image is on the form of an internar page, so we need to add \n",
    "                # the website url.\n",
    "                if (linkImage.find(\"http\") == -1 or linkImage.find(\"https\") == 1):\n",
    "                    linkImage = url[:-1] + x.attrs[\"src\"]\n",
    "                \n",
    "                # We don't want svg files.\n",
    "                if linkImage.find(\"svg\") == -1:\n",
    "\n",
    "                    # Save image url.\n",
    "                    if linkImage not in url_images:\n",
    "                        url_images.append(linkImage)\n",
    "            except:\n",
    "                pass\n",
    "        return url_images\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Downloads the images from a list of images urls.\n",
    "    \"\"\"\n",
    "    def download_images(self, images_urls):\n",
    "        for image_url in images_urls: \n",
    "            hashImage = hashlib.sha256()\n",
    "            hashImage.update(linkImage.encode())\n",
    "            image_name = hashImage.hexdigest()\n",
    "            urllib.request.urlretrieve(image_url,\"./images/\" + image_name + \".jpg\")\n",
    "            time.sleep(secs)\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect phone numbers via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPhoneNumbers(self, text):\n",
    "        patternPhones1 = \"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones2 = \"(\\(\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones3 = \"(\\([+]\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        match1 = re.findall(patternPhones1, text)\n",
    "        match2 = re.findall(patternPhones2, text)\n",
    "        match3 = re.findall(patternPhones3, text)\n",
    "        return match1 + match2 + match3\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect emails via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getEmails(self, text):\n",
    "        patternEmails =\"[a-zA-Z0-9]+[a-zA-Z0-9.%\\-\\+]*@(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,4}\"\n",
    "        result = re.findall(patternEmails, text) \n",
    "        return result\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detects BitCoin addresses via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPayment(self, text):\n",
    "        patternBtc =\"^([13][a-km-zA-HJ-NP-Z1-9]{25,34}|bc1[ac-hj-np-zAC-HJ-NP-Z02-9]{11,71})$\"\n",
    "        result = re.findall(patternBtc, text) \n",
    "        return result\n",
    "    \n",
    "    \"\"\"\n",
    "    This function gets the formated text of soup element.\n",
    "    \"\"\"\n",
    "    def get_text(self, soup):\n",
    "        for script in soup([\"script\", \"style\"]):                   \n",
    "            script.decompose()\n",
    "        text = soup.get_text().replace(\"\\n\",\" \")\n",
    "        text = re.sub('\\s+',' ',text)\n",
    "        return text\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the language of the page.\n",
    "    \"\"\"\n",
    "    def get_language(self, text):\n",
    "        try:\n",
    "            lang = langdetect.detect(text)\n",
    "        except:\n",
    "            lang = \"en\"\n",
    "        return lang\n",
    "\n",
    "    \"\"\"\n",
    "    This function makes an attemp of finding person names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_names(self, text, nlp):\n",
    "        doc = nlp(text)\n",
    "        names = list()\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PER\" or ent.label_ == \"PERSON\": \n",
    "                if ent.text not in names:\n",
    "                    names.append(ent.text)\n",
    "        return names\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses the entity labels from spacy to find locations. It also use the re \n",
    "    library to find patterns in the text that could lead into a location or address.\n",
    "    \"\"\"\n",
    "    def find_locations(self, text, nlp):\n",
    "        localidades=[\"Usaquén\",\"Chapinero\",\"Santa Fe\",\"San Cristóbal\",\"Usme\",\"Tunjuelito\",\"Bosa\",\"Kennedy\",\"Fontibón\",\"Engativá\",\n",
    "                     \"Suba\",\"Barrios Unidos\",\"Teusaquillo\",\"Los Mártires\",\"Antonio Nariño\",\"Puente Aranda\",\"La Candelaria\",\n",
    "                     \"Rafael Uribe Uribe\",\"Ciudad Bolívar\",\"Sumapaz\"]\n",
    "        cardinales=[\"Norte\",\"Sur\",\"Este\",\"Oeste\",\"Occidente\",\"Oriente\"]\n",
    "        direccion=[\"Calle\",\"Avenida\", \"Carrera\",\"Diagonal\"]\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        list_ = list()\n",
    "        for ent in doc.ents:\n",
    "            try: \n",
    "                if ent.label_==\"LOC\" and (ent.text not in list_):\n",
    "                    list_.append(ent.text)\n",
    "            except: \n",
    "                pass\n",
    "                \n",
    "        for l in localidades:\n",
    "            try: \n",
    "                locations = re.findall(\"(\"+l+\")\", text)[0]\n",
    "                if len(locations) > 0 and (locations not in list_):\n",
    "                    list_.append(locations)\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        for c in cardinales:\n",
    "            try: \n",
    "                locations = re.findall(\"(\"+c+\")\", text)[0]\n",
    "                if len(locations) > 0 and (locations not in list_):\n",
    "                    list_.append(locations)\n",
    "            except: \n",
    "                pass\n",
    "                \n",
    "        for d in direccion:\n",
    "            try: \n",
    "                locations = re.findall(\"(\"+d+\"[0-9]{2}\\s)\", text)[0]\n",
    "                if len(locations) > 0 and (locations not in list_):\n",
    "                    list_.append(locations)\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        return list_\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find organizations\n",
    "    \"\"\"\n",
    "    def find_organizations(self, text, nlp):\n",
    "        doc = nlp(text)\n",
    "        list_ =[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"ORG\" and (ent.text not in list_):\n",
    "                list_.append(ent.text)\n",
    "        return list_\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find dates.It also use the re library to find patterns in the text\n",
    "    that could lead in to a date.\n",
    "    \"\"\"\n",
    "    def find_dates(self, text, nlp):\n",
    "        months=[\"Enero\",\"Ene\", \"January\",\"Jan\",\"Febrero\",\"February\",\"Feb\",\"Marzo\",\"March\",\"Mar\",\"Abril\",\"April\",\"Mayo\",\"May\",'Junio','June', \"Jun\",\"Julio\",\"July\", \"Jul\",\n",
    "               \"Agosto\",\"Ago\",\"August\",\"Aug\",\"Septiembre\",'September',\"Sep\",'Octubre','October',\"Oct\",\"Noviembre\",'November',\"Nov\",\"Diciembre\",\"December\", \"Dec\"]\n",
    "        doc = nlp(text)\n",
    "        lista=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"DATE\":\n",
    "                #print(ent.text, ent.label_)\n",
    "                lista.append(ent)\n",
    "        for m in months:\n",
    "            if(len(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))>0):\n",
    "                lista.append(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))       \n",
    "        \n",
    "        return lista\n",
    "\n",
    "    \"\"\"\n",
    "    Find all website links and related social media and phonenumbers \n",
    "    associated with urls on the page.\n",
    "    Input: \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all the urls in the website.\n",
    "    def find_website_links(self, url, soup):\n",
    "        \n",
    "        # Get social media names.\n",
    "        f = open(TheWitness.social_media_names_file, \"r\")\n",
    "        sm_keywords = f.read().replace(\"\\n\", \"\").split(\" \")\n",
    "        f.close()\n",
    "\n",
    "        # Remove last / in url if exists.\n",
    "        if(url[-1] == \"/\"):\n",
    "            url = url[0: -1]\n",
    "\n",
    "        # Initialize lists.\n",
    "        internal_website_urls = list()\n",
    "        external_website_urls = list()\n",
    "        social_networks_urls = list()\n",
    "        whatsapp_cellphones = list()\n",
    "        images_urls = list()\n",
    "        emails_list = list()\n",
    "\n",
    "        for tag in soup.find_all(\"a\"):\n",
    "\n",
    "            try:\n",
    "                # Find href attribute.\n",
    "                link = tag.attrs[\"href\"]\n",
    "                \n",
    "                # If http or https is contain in the link.\n",
    "                if (link.find(\"http\") != -1 or link.find(\"https\") != -1) and link.find(\"javascript\") == -1:\n",
    "\n",
    "                    # If link is not an images or #\n",
    "                    if link.find(\"jpg\") == -1 and link.find(\"png\") == -1: \n",
    "\n",
    "                        # Determine if link corresponds to an internal navegation page.\n",
    "                        if link.find(\"/\") == 0: \n",
    "                            new_link = url + link\n",
    "                            if new_link not in internal_website_urls: \n",
    "                                internal_website_urls.append(new_link)\n",
    "\n",
    "                        # External link.\n",
    "                        else: \n",
    "\n",
    "                            # Check if the external link is a social network link!\n",
    "                            social_network_website = False\n",
    "                            for social_network_name in sm_keywords: \n",
    "                                if link.find(social_network_name) != -1: \n",
    "                                    social_network_website = True\n",
    "                                    break\n",
    "\n",
    "                            if social_network_website:\n",
    "\n",
    "                                if (link not in social_networks_urls): \n",
    "\n",
    "                                    # Add link to social networks list.\n",
    "                                    social_networks_urls.append(link)\n",
    "\n",
    "\n",
    "                                    # If link is of the form api.whatsapp.com, get the phonenumber.\n",
    "                                    if link.find(\"whatsapp\") != -1:\n",
    "\n",
    "                                        number = re.findall(\"phone=([0-9]*)\", link)\n",
    "                                        if number[0] not in whatsapp_cellphones: \n",
    "                                            whatsapp_cellphones.append(number[0])     \n",
    "\n",
    "                            # If link is not a social network link, just add it to website_urls list.\n",
    "                            else: \n",
    "\n",
    "                                # We have an email!\n",
    "                                if link.find(\"mailto:\") != -1:\n",
    "                                    if link not in emails_list: \n",
    "                                        emails_list.append(link)\n",
    "                                elif link not in external_website_urls: \n",
    "                                    external_website_urls.append(link)\n",
    "\n",
    "                    # If the link is an image.\n",
    "                    elif link.find(\"jpg\") != -1 or link.find(\"png\") != -1:\n",
    "                        if link not in images_urls: \n",
    "                            images_urls.append(link)\n",
    "\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        return internal_website_urls, external_website_urls, emails_list, social_networks_urls, whatsapp_cellphones, images_urls\n",
    "\n",
    "    ## -------------------\n",
    "    ## Auxiliar functions.\n",
    "    ## -------------------\n",
    "\n",
    "    \"\"\"\n",
    "    This method joins to python's lists.\n",
    "    \"\"\"   \n",
    "    def join_lists(self, list1, list2): \n",
    "        new_list = list()\n",
    "        for element in list1: \n",
    "            new_list.append(element)\n",
    "        for element in list2: \n",
    "            new_list.append(element)\n",
    "        return new_list\n",
    "    \n",
    "    \"\"\"\n",
    "    This method return url roots\n",
    "    \"\"\"\n",
    "    \n",
    "    def clean_urls(self, urls):\n",
    "        final_urls = []\n",
    "        for url in urls:\n",
    "            hostname = urllib.parse.urlparse(url).hostname\n",
    "            if hostname not in final_urls:\n",
    "                final_urls.append(hostname)\n",
    "        return final_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Witness ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"The Witness ready!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
