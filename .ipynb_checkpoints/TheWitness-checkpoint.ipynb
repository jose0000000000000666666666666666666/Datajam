{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The witness class.\n",
    "class TheWitness: \n",
    "    \n",
    "    # Static variables.\n",
    "    headers = {}\n",
    "    headers[\"User-Agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "    social_media_names_file = \"./datasets/socialmedia_names.txt\"\n",
    "    \n",
    "    # Initialize witness features.\n",
    "    def __init__(self, origin_url, destination_url, destination_hash, \n",
    "                 distance_from_root):\n",
    "        \n",
    "        # Where was the agent created?\n",
    "        self.agent_origin = origin_url\n",
    "        self.agent_destination = destination_url\n",
    "        self.agent_session = requests.session()\n",
    "        self.destination_hash = destination_hash\n",
    "        self.distance_from_root = distance_from_root\n",
    "        \n",
    "    \n",
    "    # Scrap the destination website!\n",
    "    def scrap(self):\n",
    "        \n",
    "        # Get soup object.\n",
    "        soup = self.getSoupObject()\n",
    "        \n",
    "        # Find all links on the page!\n",
    "        result = self.find_website_links(self.agent_destination, soup)\n",
    "        \n",
    "        internal_website_urls = result[0]\n",
    "        external_website_urls = result[1]\n",
    "        urls = self.join_lists(internal_website_urls, external_website_urls)\n",
    "        emails_list_1 = result[2]\n",
    "        social_networks_urls = result[3]\n",
    "        whatsapp_cellphones = result[4]\n",
    "        images_urls_1 = result[5]\n",
    "        \n",
    "        # Get text of the website.\n",
    "        text = self.get_text(soup)\n",
    "        \n",
    "        # Get language of the website.\n",
    "        language = self.get_language(text)\n",
    "        \n",
    "        if language == \"en\":\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            print(\"en\")\n",
    "        elif language == \"es\":\n",
    "            nlp = spacy.load(\"es_core_news_sm\")\n",
    "            print(\"es\")\n",
    "        \n",
    "        # Get names that appear on the website.\n",
    "        names = self.get_names(text, nlp)\n",
    "        \n",
    "        # Get locations\n",
    "        \n",
    "        locations = self.find_locations(text)\n",
    "        \n",
    "        # Get coordinates of maps that appear on the website.\n",
    "        coordinates = []\n",
    "        \n",
    "        # Get orgazations name's that appear on the website.\n",
    "        organizations = self.find_organizations(text)\n",
    "        \n",
    "        # Get dates that appear on the website.\n",
    "        dates = self.find_dates(text)\n",
    "        \n",
    "        # Get all phone numbers that appear on the website.\n",
    "        phonenumbers = self.getPhoneNumbers(text)\n",
    "        \n",
    "        # Get images urls that appear on the website. \n",
    "        images_urls = self.getImagesFromWebPage(url, download=False, secs=0)\n",
    "        \n",
    "        # Get payment accounts that appear on the website.\n",
    "        payment_accounts = self.getPayment(text)\n",
    "        \n",
    "        # Get emails that appear on the website.\n",
    "        emails = self.getEmails(text)\n",
    "        \n",
    "        return [self.destination_hash, # ID of the website.\n",
    "                self.agent_destination, # URL of the website.\n",
    "                urls,                  # URLS found on the website.\n",
    "                social_networks_urls,  # Social networks urls.\n",
    "                language,              # Language of the website.\n",
    "                text,                  # Text of the website.\n",
    "                names,                 # Names that appeared on the website.\n",
    "                locations,             # Locations mentioned on website.\n",
    "                coordinates,           # Coordinates of maps that appear on the website.\n",
    "                organizations,         # Organization name's.\n",
    "                dates,                 # Dates on the website.\n",
    "                phonenumbers,          # Phonenumbers on the website.\n",
    "                images_urls,           # Images urls.\n",
    "                payment_accounts,      # Payment Accounts (banks, crytpo).\n",
    "                emails                 # Emails on the website.\n",
    "               ]\n",
    "        \n",
    "    ## -----------------------\n",
    "    ## Webscrapping functions.\n",
    "    ## -----------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This function saves all the images urls and downloads them optionaly\n",
    "    \"\"\"\n",
    "    def getImagesFromWebPage(self, url, download=False, secs=0 ):\n",
    "\n",
    "        r = requests.get(url)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        images=soup.find_all(\"img\")\n",
    "        urlImages=[]\n",
    "        for x in soup.find_all(\"img\"):\n",
    "            try:\n",
    "                linkImage=url[:-1]+x.attrs[\"src\"]\n",
    "                hashImage=hashlib.sha256()\n",
    "                hashImage.update(linkImage.encode())\n",
    "                urlImages.append([linkImage,hashImage.hexdigest()])\n",
    "                if download:\n",
    "\n",
    "                    urllib.request.urlretrieve(linkImage,\"./images/\"+hashImage.hexdigest()+\".jpg\")\n",
    "                    time.sleep(secs)\n",
    "            except:\n",
    "                pass\n",
    "        return urlImages\n",
    "     \n",
    "    \"\"\"\n",
    "    This function detect phone numbers via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPhoneNumbers(self, text):\n",
    "        patternPhones1 = \"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones2 = \"(\\(\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones3 = \"(\\([+]\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        match1 = re.findall(patternPhones1, text)\n",
    "        match2 = re.findall(patternPhones2, text)\n",
    "        match3 = re.findall(patternPhones3, text)\n",
    "        return match1+match2+match3\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect emails via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getEmails(self, text):\n",
    "        \n",
    "        patternEmails =\"[a-zA-Z0-9]+[a-zA-Z0-9.%\\-\\+]*@(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,4}\"\n",
    "\n",
    "        result = re.findall(patternEmails, text) \n",
    "\n",
    "        return(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect BitCoin adress via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPayment(self, text):\n",
    "        \n",
    "        patternBtc =\"^([13][a-km-zA-HJ-NP-Z1-9]{25,34}|bc1[ac-hj-np-zAC-HJ-NP-Z02-9]{11,71})$\"\n",
    "\n",
    "        result = re.findall(patternBtc, text) \n",
    "\n",
    "        print(result)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This functions makes a requests to the destination website and\n",
    "    initialize a BeautifulSoup object for processing it.\n",
    "    \"\"\"\n",
    "    def getSoupObject(self):\n",
    "        r = self.agent_session.get(self.agent_destination, headers = TheWitness.headers)\n",
    "        data = r.text\n",
    "        return BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This function gets the formated text of soup element.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_text(self, soup):\n",
    "        for script in soup([\"script\", \"style\"]):                   \n",
    "            script.decompose()\n",
    "        text = soup.get_text().replace(\"\\n\",\" \")\n",
    "        text = re.sub('\\s+',' ',text)\n",
    "        return text\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the language of the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_language(self, text):\n",
    "        lang = langdetect.detect(text)\n",
    "#         lang = langs[langs[\"ISO Code\"] == lang][\"Language\"]\n",
    "        return lang\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    This function makes an attemp of finding person names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_names(self, text, nlp):\n",
    "        doc = nlp(text)\n",
    "        person = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PER\":\n",
    "                person.append(ent)\n",
    "        return person\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses the entity labels from spacy to find locations. It also use the re library to find patterns in the text\n",
    "    that could lead in to a location or address\n",
    "    \"\"\"\n",
    "    def find_locations(self, text):\n",
    "        localidades=[\"Usaquén\",\"Chapinero\",\"Santa Fe\",\"San Cristóbal\",\"Usme\",\"Tunjuelito\",\"Bosa\",\"Kennedy\",\"Fontibón\",\"Engativá\",\n",
    "                     \"Suba\",\"Barrios Unidos\",\"Teusaquillo\",\"Los Mártires\",\"Antonio Nariño\",\"Puente Aranda\",\"La Candelaria\",\n",
    "                     \"Rafael Uribe Uribe\",\"Ciudad Bolívar\",\"Sumapaz\"]\n",
    "        cardinales=[\"Norte\",\"Sur\",\"Este\",\"Oeste\",\"Occidente\",\"Oriente\"]\n",
    "        direccion=[\"Calle\",\"Avenida\", \"Carrera\",\"Diagonal\"]\n",
    "        doc = nlp(text)\n",
    "        lista=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"LOC\":\n",
    "                print(ent.text, ent.label_)\n",
    "                lista.append(ent)\n",
    "        for l in localidades:\n",
    "            if(len(re.findall(\"(\"+l+\")\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+l+\")\", text), text)\n",
    "        for c in cardinales:\n",
    "            if(len(re.findall(\"(\"+c+\")\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+c+\")\", text), text)\n",
    "        for d in direccion:\n",
    "            if(len(re.findall(\"(\"+d+\"[0-9]{2}\\s)\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+d+\"[0-9]{2}\\s)\", text), text)\n",
    "        return lista\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find organizations\n",
    "    \"\"\"\n",
    "    def find_organizations(text):\n",
    "        doc = nlp(text)\n",
    "        list=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"ORG\":\n",
    "                print(ent.text, ent.label_)\n",
    "                list.append(ent)\n",
    "            \n",
    "        return list\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find dates.It also use the re library to find patterns in the text\n",
    "    that could lead in to a date.\n",
    "    \"\"\"\n",
    "    def find_dates(text):\n",
    "        months=[\"Enero\",\"Ene\", \"January\",\"Jan\",\"Febrero\",\"February\",\"Feb\",\"Marzo\",\"March\",\"Mar\",\"Abril\",\"April\",\"Mayo\",\"May\",'Junio','June', \"Jun\",\"Julio\",\"July\", \"Jul\",\n",
    "               \"Agosto\",\"Ago\",\"August\",\"Aug\",\"Septiembre\",'September',\"Sep\",'Octubre','October',\"Oct\",\"Noviembre\",'November',\"Nov\",\"Diciembre\",\"December\", \"Dec\"]\n",
    "        doc = nlp(text)\n",
    "        lista=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"DATE\":\n",
    "                print(ent.text, ent.label_)\n",
    "                lista.append(ent)\n",
    "        for m in months:\n",
    "            if(len(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))>0):\n",
    "                lista.append(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))       \n",
    "        \n",
    "        return lista\n",
    "\n",
    "    \"\"\"\n",
    "    Find all website links and related social media and phonenumbers \n",
    "    associated with urls on the page.\n",
    "    Input: \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all the urls in the website.\n",
    "    def find_website_links(self, url, soup):\n",
    "        \n",
    "        # Get social media names.\n",
    "        f = open(TheWitness.social_media_names_file, \"r\")\n",
    "        sm_keywords = f.read().replace(\"\\n\", \"\").split(\" \")\n",
    "        f.close()\n",
    "\n",
    "        # Remove last / in url if exists.\n",
    "        if(url[-1] == \"/\"):\n",
    "            url = url[0: -1]\n",
    "\n",
    "        # Initialize lists.\n",
    "        internal_website_urls = list()\n",
    "        external_website_urls = list()\n",
    "        social_networks_urls = list()\n",
    "        whatsapp_cellphones = list()\n",
    "        images_urls = list()\n",
    "        emails_list = list()\n",
    "\n",
    "        for tag in soup.find_all(\"a\"):\n",
    "\n",
    "            try:\n",
    "                # Find href attribute.\n",
    "                link = tag.attrs[\"href\"]\n",
    "\n",
    "\n",
    "                # If link is not an images or #\n",
    "                if link.find(\"#\") == -1 and link.find(\"jpg\") == -1 and link.find(\"png\") == -1: \n",
    "\n",
    "                    # Determine if link corresponds to an internal navegation page.\n",
    "                    if link.find(\"/\") == 0: \n",
    "                        new_link = url + link\n",
    "                        if new_link not in internal_website_urls: \n",
    "                            internal_website_urls.append(new_link)\n",
    "\n",
    "                    # External link.\n",
    "                    else: \n",
    "\n",
    "                        # Check if the external link is a social network link!\n",
    "                        social_network_website = False\n",
    "                        for social_network_name in sm_keywords: \n",
    "                            if link.find(social_network_name) != -1: \n",
    "                                social_network_website = True\n",
    "                                break\n",
    "\n",
    "                        if social_network_website:\n",
    "\n",
    "                            if (link not in social_networks_urls): \n",
    "\n",
    "                                # Add link to social networks list.\n",
    "                                social_networks_urls.append(link)\n",
    "\n",
    "\n",
    "                                # If link is of the form api.whatsapp.com, get the phonenumber.\n",
    "                                if link.find(\"whatsapp\") != -1:\n",
    "\n",
    "                                    number = re.findall(\"phone=([0-9]*)\", link)\n",
    "                                    if number[0] not in whatsapp_cellphones: \n",
    "                                        whatsapp_cellphones.append(number[0])     \n",
    "\n",
    "                        # If link is not a social network link, just add it to website_urls list.\n",
    "                        else: \n",
    "\n",
    "                            # We have an email!\n",
    "                            if link.find(\"mailto:\") != -1:\n",
    "                                if link not in emails_list: \n",
    "                                    emails_list.append(link)\n",
    "                            elif link not in external_website_urls: \n",
    "                                external_website_urls.append(link)\n",
    "\n",
    "                # If the link is an image.\n",
    "                elif link.find(\"jpg\") != -1 or link.find(\"png\") != -1:\n",
    "                    if link not in images_urls: \n",
    "                        images_urls.append(link)\n",
    "\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        return internal_website_urls, external_website_urls, emails_list, social_networks_urls, whatsapp_cellphones, images_urls\n",
    "\n",
    "    ## -------------------\n",
    "    ## Auxiliar functions.\n",
    "    ## -------------------\n",
    "\n",
    "    \"\"\"\n",
    "    This method joins to python's lists.\n",
    "    \"\"\"   \n",
    "    def join_lists(self, list1, list2): \n",
    "        new_list = list()\n",
    "        for element in list1: \n",
    "            new_list.append(element)\n",
    "        for element in list2: \n",
    "            new_list.append(element)\n",
    "        return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
