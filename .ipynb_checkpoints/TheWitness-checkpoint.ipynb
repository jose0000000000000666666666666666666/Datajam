{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebScrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection libraries.\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "#from langdetect import detect\n",
    "import re\n",
    "import hashlib \n",
    "import time\n",
    "import langdetect\n",
    "\n",
    "import spacy\n",
    "import es_core_news_sm\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The witness class.\n",
    "class TheWitness: \n",
    "    \n",
    "    # Static variables.\n",
    "    headers = {}\n",
    "    headers[\"User-Agent\"] = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\"\n",
    "    social_media_names_file = \"./datasets/socialmedia_names.txt\"\n",
    "    \n",
    "    # Initialize witness features.\n",
    "    def __init__(self, origin_url, destination_url, destination_hash, \n",
    "                 distance_from_root):\n",
    "        \n",
    "        # Where was the agent created?\n",
    "        self.agent_origin = origin_url\n",
    "        self.agent_destination = destination_url\n",
    "        self.agent_session = requests.session()\n",
    "        self.destination_hash = destination_hash\n",
    "        self.distance_from_root = distance_from_root\n",
    "        \n",
    "    \n",
    "    # Scrap the destination website!\n",
    "    def scrap(self):\n",
    "        \n",
    "        # Get soup object.\n",
    "        soup = self.getSoupObject()\n",
    "        \n",
    "        # Find all links on the page!\n",
    "        result = self.find_website_links(self.agent_destination, soup)\n",
    "        \n",
    "        internal_website_urls = result[0]\n",
    "        external_website_urls = result[1]\n",
    "        urls = self.join_lists(internal_website_urls, external_website_urls)\n",
    "        emails_list_1 = result[2]\n",
    "        social_networks_urls = result[3]\n",
    "        whatsapp_cellphones = result[4]\n",
    "        images_urls_1 = result[5]\n",
    "        \n",
    "        # Get text of the website.\n",
    "        text = self.get_text(soup)\n",
    "        \n",
    "        # Get language of the website.\n",
    "        language = self.get_language(text)\n",
    "        \n",
    "        if language == \"en\":\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "            print(\"en\")\n",
    "        elif language == \"es\":\n",
    "            nlp = spacy.load(\"es_core_news_sm\")\n",
    "            print(\"es\")\n",
    "        \n",
    "        # Get names that appear on the website.\n",
    "        names = self.get_names(text, nlp)\n",
    "        \n",
    "        # Get locations\n",
    "        \n",
    "        locations = self.find_locations(text)\n",
    "        \n",
    "        # Get coordinates of maps that appear on the website.\n",
    "        coordinates = []\n",
    "        \n",
    "        # Get orgazations name's that appear on the website.\n",
    "        organizations = []\n",
    "        \n",
    "        # Get dates that appear on the website.\n",
    "        dates = []\n",
    "        \n",
    "        # Get all phone numbers that appear on the website.\n",
    "        phonenumbers = self.getPhoneNumbers(text)\n",
    "        \n",
    "        # Get images urls that appear on the website. \n",
    "        images_urls = self.getImagesFromWebPage(url, download=False, secs=0)\n",
    "        \n",
    "        # Get payment accounts that appear on the website.\n",
    "        payment_accounts = self.getPayment(text)\n",
    "        \n",
    "        # Get emails that appear on the website.\n",
    "        emails = self.getEmails(text)\n",
    "        \n",
    "        return [self.destination_hash, # ID of the website.\n",
    "                self.agent_destination, # URL of the website.\n",
    "                urls,                  # URLS found on the website.\n",
    "                social_networks_urls,  # Social networks urls.\n",
    "                language,              # Language of the website.\n",
    "                text,                  # Text of the website.\n",
    "                names,                 # Names that appeared on the website.\n",
    "                locations,             # Locations mentioned on website.\n",
    "                coordinates,           # Coordinates of maps that appear on the website.\n",
    "                organizations,         # Organization name's.\n",
    "                dates,                 # Dates on the website.\n",
    "                phonenumbers,          # Phonenumbers on the website.\n",
    "                images_urls,           # Images urls.\n",
    "                payment_accounts,      # Payment Accounts (banks, crytpo).\n",
    "                emails                 # Emails on the website.\n",
    "               ]\n",
    "        \n",
    "    ## -----------------------\n",
    "    ## Webscrapping functions.\n",
    "    ## -----------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This function saves all the images urls and downloads them optionaly\n",
    "    \"\"\"\n",
    "    def getImagesFromWebPage(self, url, download=False, secs=0 ):\n",
    "\n",
    "        r = requests.get(url)\n",
    "        data = r.text\n",
    "        soup = BeautifulSoup(data, \"html.parser\")\n",
    "        images=soup.find_all(\"img\")\n",
    "        urlImages=[]\n",
    "        for x in soup.find_all(\"img\"):\n",
    "            try:\n",
    "                linkImage=url[:-1]+x.attrs[\"src\"]\n",
    "                hashImage=hashlib.sha256()\n",
    "                hashImage.update(linkImage.encode())\n",
    "                urlImages.append([linkImage,hashImage.hexdigest()])\n",
    "                if download:\n",
    "\n",
    "                    urllib.request.urlretrieve(linkImage,\"./images/\"+hashImage.hexdigest()+\".jpg\")\n",
    "                    time.sleep(secs)\n",
    "            except:\n",
    "                pass\n",
    "        return urlImages\n",
    "     \n",
    "    \"\"\"\n",
    "    This function detect phone numbers via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPhoneNumbers(self, text):\n",
    "        patternPhones1 = \"(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones2 = \"(\\(\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        patternPhones3 = \"(\\([+]\\d{1}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{2}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\([+]\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4})\"\n",
    "        match1 = re.findall(patternPhones1, text)\n",
    "        match2 = re.findall(patternPhones2, text)\n",
    "        match3 = re.findall(patternPhones3, text)\n",
    "        return match1+match2+match3\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect emails via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getEmails(self, text):\n",
    "        \n",
    "        patternEmails =\"[a-zA-Z0-9]+[a-zA-Z0-9.%\\-\\+]*@(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,4}\"\n",
    "\n",
    "        result = re.findall(patternEmails, text) \n",
    "\n",
    "        return(result)\n",
    "    \n",
    "    \"\"\"\n",
    "    This function detect BitCoin adress via regular expressions (regex)\n",
    "    \"\"\"\n",
    "    def getPayment(self, text):\n",
    "        \n",
    "        patternBtc =\"^([13][a-km-zA-HJ-NP-Z1-9]{25,34}|bc1[ac-hj-np-zAC-HJ-NP-Z02-9]{11,71})$\"\n",
    "\n",
    "        result = re.findall(patternBtc, text) \n",
    "\n",
    "        print(result)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This functions makes a requests to the destination website and\n",
    "    initialize a BeautifulSoup object for processing it.\n",
    "    \"\"\"\n",
    "    def getSoupObject(self):\n",
    "        r = self.agent_session.get(self.agent_destination, headers = TheWitness.headers)\n",
    "        data = r.text\n",
    "        return BeautifulSoup(data, \"html.parser\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    This function gets the formated text of soup element.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_text(self, soup):\n",
    "        for script in soup([\"script\", \"style\"]):                   \n",
    "            script.decompose()\n",
    "        text = soup.get_text().replace(\"\\n\",\" \")\n",
    "        text = re.sub('\\s+',' ',text)\n",
    "        return text\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns the language of the page.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_language(self, text):\n",
    "        lang = langdetect.detect(text)\n",
    "#         lang = langs[langs[\"ISO Code\"] == lang][\"Language\"]\n",
    "        return lang\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    This function makes an attemp of finding person names.\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_names(self, text, nlp):\n",
    "        doc = nlp(text)\n",
    "        person = []\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PER\":\n",
    "                person.append(ent)\n",
    "        return person\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses the entity labels from spacy to find locations. It also use the re library to find patterns in the text\n",
    "    that could lead in to a location or address\n",
    "    \"\"\"\n",
    "    def find_locations(self, text):\n",
    "        localidades=[\"Usaquén\",\"Chapinero\",\"Santa Fe\",\"San Cristóbal\",\"Usme\",\"Tunjuelito\",\"Bosa\",\"Kennedy\",\"Fontibón\",\"Engativá\",\n",
    "                     \"Suba\",\"Barrios Unidos\",\"Teusaquillo\",\"Los Mártires\",\"Antonio Nariño\",\"Puente Aranda\",\"La Candelaria\",\n",
    "                     \"Rafael Uribe Uribe\",\"Ciudad Bolívar\",\"Sumapaz\"]\n",
    "        cardinales=[\"Norte\",\"Sur\",\"Este\",\"Oeste\",\"Occidente\",\"Oriente\"]\n",
    "        direccion=[\"Calle\",\"Avenida\", \"Carrera\",\"Diagonal\"]\n",
    "        doc = nlp(text)\n",
    "        list=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"LOC\":\n",
    "                print(ent.text, ent.label_)\n",
    "                list.append(ent)\n",
    "        for l in localidades:\n",
    "            if(len(re.findall(\"(\"+l+\")\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+l+\")\", text), text)\n",
    "        for c in cardinales:\n",
    "            if(len(re.findall(\"(\"+c+\")\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+c+\")\", text), text))\n",
    "        for d in direccion:\n",
    "            if(len(re.findall(\"(\"+d+\"[0-9]{2}\\s)\", text))>0):\n",
    "                lista.append(re.findall(\"(\"+d+\"[0-9]{2}\\s)\", text), text)) \n",
    "        return list\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find organizations\n",
    "    \"\"\"\n",
    "    def find_organizations(text):\n",
    "        doc = nlp(text)\n",
    "        list=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"ORG\":\n",
    "                print(ent.text, ent.label_)\n",
    "                list.append(ent)\n",
    "            \n",
    "        return list\n",
    "    \n",
    "    \"\"\"\n",
    "    This function uses of entity labels from spacy to find dates.It also use the re library to find patterns in the text\n",
    "    that could lead in to a date.\n",
    "    \"\"\"\n",
    "    def find_dates(text):\n",
    "        months=[\"Enero\",\"Ene\", \"January\",\"Jan\",\"Febrero\",\"February\",\"Feb\",\"Marzo\",\"March\",\"Mar\",\"Abril\",\"April\",\"Mayo\",\"May\",'Junio','June', \"Jun\",\"Julio\",\"July\", \"Jul\",\n",
    "               \"Agosto\",\"Ago\",\"August\",\"Aug\",\"Septiembre\",'September',\"Sep\",'Octubre','October',\"Oct\",\"Noviembre\",'November',\"Nov\",\"Diciembre\",\"December\", \"Dec\"]\n",
    "        doc = nlp(text)\n",
    "        lista=[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_==\"DATE\":\n",
    "                print(ent.text, ent.label_)\n",
    "                lista.append(ent)\n",
    "        for m in months:\n",
    "            if(len(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))>0):\n",
    "                lista.append(re.findall(\"([0-9]{2}\\s\"+m+\"\\s[0-9]{4})\", text))       \n",
    "        \n",
    "        return lista\n",
    "\n",
    "    \"\"\"\n",
    "    Find all website links and related social media and phonenumbers \n",
    "    associated with urls on the page.\n",
    "    Input: \n",
    "    Output:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find all the urls in the website.\n",
    "    def find_website_links(self, url, soup):\n",
    "        \n",
    "        # Get social media names.\n",
    "        f = open(TheWitness.social_media_names_file, \"r\")\n",
    "        sm_keywords = f.read().replace(\"\\n\", \"\").split(\" \")\n",
    "        f.close()\n",
    "\n",
    "        # Remove last / in url if exists.\n",
    "        if(url[-1] == \"/\"):\n",
    "            url = url[0: -1]\n",
    "\n",
    "        # Initialize lists.\n",
    "        internal_website_urls = list()\n",
    "        external_website_urls = list()\n",
    "        social_networks_urls = list()\n",
    "        whatsapp_cellphones = list()\n",
    "        images_urls = list()\n",
    "        emails_list = list()\n",
    "\n",
    "        for tag in soup.find_all(\"a\"):\n",
    "\n",
    "            try:\n",
    "                # Find href attribute.\n",
    "                link = tag.attrs[\"href\"]\n",
    "\n",
    "\n",
    "                # If link is not an images or #\n",
    "                if link.find(\"#\") == -1 and link.find(\"jpg\") == -1 and link.find(\"png\") == -1: \n",
    "\n",
    "                    # Determine if link corresponds to an internal navegation page.\n",
    "                    if link.find(\"/\") == 0: \n",
    "                        new_link = url + link\n",
    "                        if new_link not in internal_website_urls: \n",
    "                            internal_website_urls.append(new_link)\n",
    "\n",
    "                    # External link.\n",
    "                    else: \n",
    "\n",
    "                        # Check if the external link is a social network link!\n",
    "                        social_network_website = False\n",
    "                        for social_network_name in sm_keywords: \n",
    "                            if link.find(social_network_name) != -1: \n",
    "                                social_network_website = True\n",
    "                                break\n",
    "\n",
    "                        if social_network_website:\n",
    "\n",
    "                            if (link not in social_networks_urls): \n",
    "\n",
    "                                # Add link to social networks list.\n",
    "                                social_networks_urls.append(link)\n",
    "\n",
    "\n",
    "                                # If link is of the form api.whatsapp.com, get the phonenumber.\n",
    "                                if link.find(\"whatsapp\") != -1:\n",
    "\n",
    "                                    number = re.findall(\"phone=([0-9]*)\", link)\n",
    "                                    if number[0] not in whatsapp_cellphones: \n",
    "                                        whatsapp_cellphones.append(number[0])     \n",
    "\n",
    "                        # If link is not a social network link, just add it to website_urls list.\n",
    "                        else: \n",
    "\n",
    "                            # We have an email!\n",
    "                            if link.find(\"mailto:\") != -1:\n",
    "                                if link not in emails_list: \n",
    "                                    emails_list.append(link)\n",
    "                            elif link not in external_website_urls: \n",
    "                                external_website_urls.append(link)\n",
    "\n",
    "                # If the link is an image.\n",
    "                elif link.find(\"jpg\") != -1 or link.find(\"png\") != -1:\n",
    "                    if link not in images_urls: \n",
    "                        images_urls.append(link)\n",
    "\n",
    "            except: \n",
    "                pass\n",
    "\n",
    "        return internal_website_urls, external_website_urls, emails_list, social_networks_urls, whatsapp_cellphones, images_urls\n",
    "\n",
    "    ## -------------------\n",
    "    ## Auxiliar functions.\n",
    "    ## -------------------\n",
    "\n",
    "    \"\"\"\n",
    "    This method joins to python's lists.\n",
    "    \"\"\"   \n",
    "    def join_lists(self, list1, list2): \n",
    "        new_list = list()\n",
    "        for element in list1: \n",
    "            new_list.append(element)\n",
    "        for element in list2: \n",
    "            new_list.append(element)\n",
    "        return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = TheWitness(\"https://dentalgomez.co/\", \"https://dentalgomez.co/\", destination_hash = \"srgh2314\", distance_from_root = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n",
      "[Dental Gómez, Pago Múltiples, Instagram Mantente, Correo Contáctate]\n"
     ]
    }
   ],
   "source": [
    "# for element in agent.scrap(): \n",
    "#     print(element)\n",
    "print(agent.scrap()[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Omit pages.\n",
    "omit_pages = [\"google\", \"linkto\", \"odoo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables.\n",
    "hashes_map = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nodeID' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-1662b9c13528>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://modelosalacarta.com/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdestination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://modelosalacarta.com/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhash_destination\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnodeID\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdistance_from_root\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhashes_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdestination\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nodeID' is not defined"
     ]
    }
   ],
   "source": [
    "root = \"https://modelosalacarta.com/\"\n",
    "destination = \"https://modelosalacarta.com/\"\n",
    "hash_destination = nodeID(destination)\n",
    "distance_from_root = 0\n",
    "hashes_map[destination] = True\n",
    "\n",
    "witness = TheWitness(\n",
    "    root, \n",
    "    destination, \n",
    "    hash_destination, \n",
    "    distance_from_root\n",
    ")\n",
    "result = witness.scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For link in links.\n",
    "def explore_network(result):\n",
    "    \n",
    "    count = 1\n",
    "    for link in result[2]: \n",
    "\n",
    "        \n",
    "        # We already explored this website.\n",
    "        try: \n",
    "            if hashes_map[link]: \n",
    "                pass\n",
    "\n",
    "        # New website!\n",
    "        except:\n",
    "            \n",
    "            navegable = True\n",
    "            for website in omit_pages: \n",
    "                if link.find(website) != -1: \n",
    "                    navegable = False\n",
    "                    \n",
    "\n",
    "            if navegable: \n",
    "                root = \"https://modelosalacarta.com/\"\n",
    "                destination = link\n",
    "                hash_destination = nodeID(destination)\n",
    "                distance_from_root = 1\n",
    "                #hashes_map[destination] = True\n",
    "\n",
    "                witness = TheWitness(\n",
    "                    root, \n",
    "                    destination, \n",
    "                    hash_destination, \n",
    "                    distance_from_root\n",
    "                )\n",
    "                result = witness.scrap()\n",
    "                \n",
    "                print(\"---------------\")\n",
    "                print(\"Agent No.\", count)\n",
    "                print(result)\n",
    "                print(\"---------------\")\n",
    "                \n",
    "                del witness\n",
    "                \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "explore_network(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = witness.scrap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url = \"https://co.mileroticos.com\"\n",
    "url = \"https://dentalgomez.co/\"\n",
    "r = requests.get(url)\n",
    "data = r.text\n",
    "soup = BeautifulSoup(data, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodeID(url): \n",
    "    m = hashlib.sha256()\n",
    "    m.update(url.encode())\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the urls in the website.\n",
    "def find_links(url, soup):\n",
    "    \n",
    "    # Get social media names.\n",
    "    f = open(\"./datasets/socialmedia_names.txt\", \"r\")\n",
    "    sm_keywords = f.read().replace(\"\\n\", \"\").split(\" \")\n",
    "    f.close()\n",
    "\n",
    "    # Remove last / in url if exists.\n",
    "    if(url[-1] == \"/\"):\n",
    "        url = url[0: -1]\n",
    "\n",
    "    # Initialize lists.\n",
    "    internal_website_urls = list()\n",
    "    external_website_urls = list()\n",
    "    social_networks_urls = list()\n",
    "    whatsapp_cellphones = list()\n",
    "    images_urls = list()\n",
    "    emails_list = list()\n",
    "\n",
    "    for tag in soup.find_all(\"a\"):\n",
    "\n",
    "        try:\n",
    "            # Find href attribute.\n",
    "            link = tag.attrs[\"href\"]\n",
    "            \n",
    "\n",
    "            # If link is not an images or #\n",
    "            if link.find(\"#\") == -1 and link.find(\"jpg\") == -1 and link.find(\"png\") == -1: \n",
    "\n",
    "                # Determine if link corresponds to an internal navegation page.\n",
    "                if link.find(\"/\") == 0: \n",
    "                    new_link = url + link\n",
    "                    if new_link not in internal_website_urls: \n",
    "                        internal_website_urls.append(new_link)\n",
    "\n",
    "                # External link.\n",
    "                else: \n",
    "\n",
    "                    print(\"External link: \", link)\n",
    "                    \n",
    "                    # Check if the external link is a social network link!\n",
    "                    social_network_website = False\n",
    "                    for social_network_name in sm_keywords: \n",
    "                        if link.find(social_network_name) != -1: \n",
    "                            social_network_website = True\n",
    "                            break\n",
    "\n",
    "                    if social_network_website:\n",
    "                    \n",
    "                        if (link not in social_networks_urls): \n",
    "\n",
    "                            # Add link to social networks list.\n",
    "                            social_networks_urls.append(link)\n",
    "\n",
    "\n",
    "                            # If link is of the form api.whatsapp.com, get the phonenumber.\n",
    "                            if link.find(\"whatsapp\") != -1:\n",
    "\n",
    "                                number = re.findall(\"phone=([0-9]*)\", link)\n",
    "                                if number[0] not in whatsapp_cellphones: \n",
    "                                    whatsapp_cellphones.append(number[0])     \n",
    "\n",
    "                    # If link is not a social network link, just add it to website_urls list.\n",
    "                    else: \n",
    "                        \n",
    "                        # We have an email!\n",
    "                        if link.find(\"mailto:\") != -1:\n",
    "                            if link not in emails_list: \n",
    "                                emails_list.append(link)\n",
    "                        elif link not in external_website_urls: \n",
    "                            external_website_urls.append(link)\n",
    "            \n",
    "            # If the link is an image.\n",
    "            elif link.find(\"jpg\") != -1 or link.find(\"png\") != -1:\n",
    "                if link not in images_urls: \n",
    "                    images_urls.append(link)\n",
    "                \n",
    "        except: \n",
    "            pass\n",
    "\n",
    "    return internal_website_urls, external_website_urls, emails_list, social_networks_urls, whatsapp_cellphones, images_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = find_links(url, soup)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeID(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "internal_links, external_links = internal_external_links(url, urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "internal_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "social_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_cellphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
